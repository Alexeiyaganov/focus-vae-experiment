"""
VAE Experiment - Training Module
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
import json
from pathlib import Path
import gc



def check_memory(stage=""):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        cached = torch.cuda.memory_reserved() / 1024**2
        print(f"   üìä GPU –ø–∞–º—è—Ç—å {stage}: {allocated:.1f}MB / {cached:.1f}MB")




# ========== –ú–û–î–ï–õ–ò ==========
class Encoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.mu = nn.Linear(256, latent_dim)
        self.logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.net(x)
        return self.mu(h), self.logvar(h)


class Decoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 784),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.net(z)


class VAE(nn.Module):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π VAE"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon, x, mu, logvar):
        BCE = nn.functional.binary_cross_entropy(recon, x.view(-1, 784), reduction='sum')
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return (BCE + KLD) / x.size(0)


class FocusVAE(nn.Module):
    """Focus-ELBO VAE - –ù–∞—à –º–µ—Ç–æ–¥"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=3, beta=0.01):
        mu_0, logvar_0 = self.encoder(x.view(-1, 784))
        batch_size = mu_0.size(0)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        mu = mu_0.unsqueeze(0).expand(k, -1, -1).clone()
        logvar = logvar_0.unsqueeze(0).expand(k, -1, -1)

        # –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞
        with torch.no_grad():
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std

            recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
            x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            mse = ((recon - x_exp) ** 2).mean(dim=-1)
            weights = torch.softmax(-mse * beta, dim=0)

            # –°–¥–≤–∏–≥ —Å—Ä–µ–¥–Ω–µ–≥–æ
            delta = (weights.unsqueeze(-1) * (z - mu)).sum(dim=0)
            mu = mu + 0.1 * delta.unsqueeze(0)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π IWAE loss
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        log_p_x = -nn.functional.binary_cross_entropy(recon, x_exp, reduction='none').sum(dim=-1)
        log_p_z = -0.5 * (z ** 2).sum(dim=-1)
        log_q_z = -0.5 * (logvar + (z - mu) ** 2 / torch.exp(logvar)).sum(dim=-1)

        log_weight = log_p_x + log_p_z - log_q_z
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)
        weight = torch.exp(log_weight - max_log_weight)

        return -torch.log(weight.mean(dim=0) + 1e-8).mean()


class IWAE(nn.Module):
    """Importance Weighted Autoencoder"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=5):
        mu, logvar = self.encoder(x.view(-1, 784))
        batch_size = mu.size(0)

        mu = mu.unsqueeze(0).expand(k, -1, -1)
        logvar = logvar.unsqueeze(0).expand(k, -1, -1)

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        log_p_x = -nn.functional.binary_cross_entropy(recon, x_exp, reduction='none').sum(dim=-1)
        log_p_z = -0.5 * (z ** 2).sum(dim=-1)
        log_q_z = -0.5 * (logvar + (z - mu) ** 2 / torch.exp(logvar)).sum(dim=-1)

        log_weight = log_p_x + log_p_z - log_q_z
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)
        weight = torch.exp(log_weight - max_log_weight)

        return -torch.log(weight.mean(dim=0) + 1e-8).mean()


class VampPriorVAE(nn.Module):
    """VampPrior - Variational Mixture of Posteriors"""

    def __init__(self, latent_dim=32, num_components=20):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim
        self.num_components = num_components

        # –ü—Å–µ–≤–¥–æ-–≤—Ö–æ–¥—ã (–æ–±—É—á–∞–µ–º—ã–µ)
        self.pseudo_inputs = nn.Parameter(torch.randn(num_components, 784))
        self.pseudo_encoder = Encoder(latent_dim)

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon_x, x, mu, logvar):
        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

        # –ü–æ–ª—É—á–∞–µ–º prior –∏–∑ –ø—Å–µ–≤–¥–æ-–≤—Ö–æ–¥–æ–≤
        pseudo_mu, pseudo_logvar = self.pseudo_encoder(self.pseudo_inputs)

        batch_size = mu.size(0)

        # –†–∞—Å—à–∏—Ä—è–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ broadcasting
        # mu: [batch_size, latent_dim] -> [batch_size, 1, latent_dim]
        mu_expanded = mu.unsqueeze(1)

        # pseudo_mu: [num_components, latent_dim] -> [1, num_components, latent_dim]
        pseudo_mu_expanded = pseudo_mu.unsqueeze(0)
        pseudo_logvar_expanded = pseudo_logvar.unsqueeze(0)

        # logvar: [batch_size, latent_dim] -> [batch_size, 1, latent_dim]
        logvar_expanded = logvar.unsqueeze(1)

        # –í—ã—á–∏—Å–ª—è–µ–º log q(z) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        # –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch_size, num_components]
        log_q_components = -0.5 * torch.sum(
            logvar_expanded +
            (mu_expanded - pseudo_mu_expanded).pow(2) / pseudo_logvar_expanded.exp() +
            pseudo_logvar_expanded,
            dim=2
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥ —Å–º–µ—Å–∏ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞)
        log_q_components = log_q_components + torch.log(
            torch.ones(self.num_components, device=mu.device) / self.num_components)

        # –õ–æ–≥–∞—Ä–∏—Ñ–º —Å—É–º–º—ã —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è log q(z)
        log_q = torch.logsumexp(log_q_components, dim=1)

        # –í—ã—á–∏—Å–ª—è–µ–º log p(z) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π prior
        log_p = -0.5 * torch.sum(logvar + mu.pow(2) + torch.log(2 * torch.tensor(np.pi, device=mu.device)), dim=1)

        # KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
        KLD = (log_q - log_p).sum()

        return (BCE + KLD) / x.size(0)


# ========== –û–ë–£–ß–ï–ù–ò–ï ==========
def train_model(model, train_loader, epochs=30, lr=3e-4, device='cuda'):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    losses = []
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        for batch_idx, (data, _) in enumerate(train_loader):
            data = data.to(device)
            optimizer.zero_grad()

            # –†–∞–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)

            elif isinstance(model, IWAE):
                loss = model.loss(data, k=3)  # IWAE —Ç–æ–ª—å–∫–æ —Å k

            elif isinstance(model, VampPriorVAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)  # VampPrior –∫–∞–∫ VAE

            elif isinstance(model, FocusVAE):
                loss = model.loss(data, k=3, beta=0.01)  # FocusVAE —Å beta

            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"      –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, Loss: {avg_loss:.2f}")

    return losses


# ========== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ==========
def evaluate_model(model, test_loader, device='cuda'):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)

            # –†–∞–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)

            elif isinstance(model, IWAE):
                loss = model.loss(data, k=3)  # IWAE –¢–û–õ–¨–ö–û –° k

            elif isinstance(model, VampPriorVAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)  # VampPrior –∫–∞–∫ VAE

            elif isinstance(model, FocusVAE):
                loss = model.loss(data, k=3, beta=0.01)  # FocusVAE —Å beta

            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")

            total_loss += loss.item()

    return total_loss / len(test_loader)


# ========== –û–°–ù–û–í–ù–û–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ ==========
def run_experiment(config):
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    config: {
        'epochs': 30,
        'batch_size': 128,
        'latent_dim': 32,
        'learning_rate': 3e-4,
        'models': ['vae', 'focus_vae']
    }
    """
    print("\n" + "=" * 60)
    print(f"üöÄ –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
    print("=" * 60)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = config.get('batch_size', 128)
    latent_dim = config.get('latent_dim', 32)
    epochs = config.get('epochs', 30)
    lr = config.get('learning_rate', 3e-4)

    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   Latent dim: {latent_dim}")
    print(f"   Batch size: {batch_size}")
    print(f"   –≠–ø–æ—Ö–∏: {epochs}")
    print(f"   –ú–æ–¥–µ–ª–∏: {config.get('models', ['vae', 'focus_vae'])}")

    # –î–∞–Ω–Ω—ã–µ
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\nüì• –î–∞–Ω–Ω—ã–µ: {len(train_dataset)} train, {len(test_dataset)} test")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'config': config,
        'device': str(device),
        'models': {}
    }

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models_to_train = config.get('models', ['vae', 'iwae', 'vamp', 'focus_vae'])

    for model_name in models_to_train:
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ: {model_name}")
        print("-" * 40)

        # ===== –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –ú–û–î–ï–õ–¨–Æ =====
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"   üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –ø–µ—Ä–µ–¥ {model_name}")
        # ==========================================

        check_memory("–¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        if model_name == 'vae':
            model = VAE(latent_dim)
        elif model_name == 'iwae':
            model = IWAE(latent_dim)
        elif model_name == 'vamp':
            model = VampPriorVAE(latent_dim)
        elif model_name == 'focus_vae':
            model = FocusVAE(latent_dim)
        else:
            print(f"   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            continue

        check_memory("–ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")

        # –û–±—É—á–µ–Ω–∏–µ
        losses = train_model(model, train_loader, epochs, lr, device)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_loss = evaluate_model(model, test_loader, device)

        print(f"   ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π Train Loss: {losses[-1]:.2f}")
        print(f"   ‚úÖ Test Loss: {test_loss:.2f}")

        results['models'][model_name] = {
            'train_losses': losses,
            'test_loss': test_loss,
            'final_train_loss': losses[-1]
        }

        check_memory("–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")

        # ===== –£–î–ê–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï =====
        del model  # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å
        gc.collect()  # –°–æ–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä Python
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
            print(f"   üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ {model_name}")
        # =================================================

        check_memory("–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è")

    print("\n" + "=" * 60)
    print(f"‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 60)

    return results
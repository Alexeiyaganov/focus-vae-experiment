"""
VAE Experiment - Training Module
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
import json
from pathlib import Path


# ========== –ú–û–î–ï–õ–ò ==========
class Encoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.mu = nn.Linear(256, latent_dim)
        self.logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.net(x)
        return self.mu(h), self.logvar(h)


class Decoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 784),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.net(z)


class VAE(nn.Module):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π VAE"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon, x, mu, logvar):
        BCE = nn.functional.binary_cross_entropy(recon, x.view(-1, 784), reduction='sum')
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return (BCE + KLD) / x.size(0)


class FocusVAE(nn.Module):
    """Focus-ELBO VAE - –ù–∞—à –º–µ—Ç–æ–¥"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=3, beta=0.01):
        mu_0, logvar_0 = self.encoder(x.view(-1, 784))
        batch_size = mu_0.size(0)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        mu = mu_0.unsqueeze(0).expand(k, -1, -1).clone()
        logvar = logvar_0.unsqueeze(0).expand(k, -1, -1)

        # –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞
        with torch.no_grad():
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std

            recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
            x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            mse = ((recon - x_exp) ** 2).mean(dim=-1)
            weights = torch.softmax(-mse * beta, dim=0)

            # –°–¥–≤–∏–≥ —Å—Ä–µ–¥–Ω–µ–≥–æ
            delta = (weights.unsqueeze(-1) * (z - mu)).sum(dim=0)
            mu = mu + 0.1 * delta.unsqueeze(0)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π IWAE loss
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        log_p_x = -nn.functional.binary_cross_entropy(recon, x_exp, reduction='none').sum(dim=-1)
        log_p_z = -0.5 * (z ** 2).sum(dim=-1)
        log_q_z = -0.5 * (logvar + (z - mu) ** 2 / torch.exp(logvar)).sum(dim=-1)

        log_weight = log_p_x + log_p_z - log_q_z
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)
        weight = torch.exp(log_weight - max_log_weight)

        return -torch.log(weight.mean(dim=0) + 1e-8).mean()


class IWAE(nn.Module):
    """Importance Weighted Autoencoder"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=5):
        mu, logvar = self.encoder(x.view(-1, 784))
        batch_size = mu.size(0)

        mu = mu.unsqueeze(0).expand(k, -1, -1)
        logvar = logvar.unsqueeze(0).expand(k, -1, -1)

        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        log_p_x = -nn.functional.binary_cross_entropy(recon, x_exp, reduction='none').sum(dim=-1)
        log_p_z = -0.5 * (z ** 2).sum(dim=-1)
        log_q_z = -0.5 * (logvar + (z - mu) ** 2 / torch.exp(logvar)).sum(dim=-1)

        log_weight = log_p_x + log_p_z - log_q_z
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)
        weight = torch.exp(log_weight - max_log_weight)

        return -torch.log(weight.mean(dim=0) + 1e-8).mean()


class VampPriorVAE(nn.Module):
    """VampPrior - Variational Mixture of Posteriors"""

    def __init__(self, latent_dim=32, num_components=20):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

        # –ü—Å–µ–≤–¥–æ-–≤—Ö–æ–¥—ã (–æ–±—É—á–∞–µ–º—ã–µ)
        self.pseudo_inputs = nn.Parameter(torch.randn(num_components, 784))
        self.pseudo_encoder = Encoder(latent_dim)

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon_x, x, mu, logvar):
        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')

        # –ü–æ–ª—É—á–∞–µ–º prior –∏–∑ –ø—Å–µ–≤–¥–æ-–≤—Ö–æ–¥–æ–≤
        pseudo_mu, pseudo_logvar = self.pseudo_encoder(self.pseudo_inputs)

        # –í—ã—á–∏—Å–ª—è–µ–º KL —Å mixture prior
        z = mu
        log_q = -0.5 * torch.sum(logvar + (z.unsqueeze(1) - pseudo_mu) ** 2 / torch.exp(pseudo_logvar) + pseudo_logvar,
                                 dim=2)
        log_q = log_q + torch.log(torch.ones(len(pseudo_mu)) / len(pseudo_mu))
        log_q = torch.logsumexp(log_q, dim=1)

        log_p = -0.5 * torch.sum(logvar + 1 + np.log(2 * np.pi), dim=1)
        KLD = (log_q - log_p).sum()

        return (BCE + KLD) / x.size(0)


# ========== –û–ë–£–ß–ï–ù–ò–ï ==========
def train_model(model, train_loader, epochs=30, lr=3e-4, device='cuda'):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    losses = []
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        for batch_idx, (data, _) in enumerate(train_loader):
            data = data.to(device)
            optimizer.zero_grad()

            # –†–∞–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)

            elif isinstance(model, IWAE):
                loss = model.loss(data, k=3)  # IWAE —Ç–æ–ª—å–∫–æ —Å k

            elif isinstance(model, VampPriorVAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)  # VampPrior –∫–∞–∫ VAE

            elif isinstance(model, FocusVAE):
                loss = model.loss(data, k=3, beta=0.01)  # FocusVAE —Å beta

            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"      –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, Loss: {avg_loss:.2f}")

    return losses


# ========== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ==========
def evaluate_model(model, test_loader, device='cuda'):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)

            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)
            else:  # FocusVAE
                loss = model.loss(data, k=3, beta=0.01)  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û

            total_loss += loss.item()

    return total_loss / len(test_loader)


# ========== –û–°–ù–û–í–ù–û–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ ==========
def run_experiment(config):
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    config: {
        'epochs': 30,
        'batch_size': 128,
        'latent_dim': 32,
        'learning_rate': 3e-4,
        'models': ['vae', 'focus_vae']
    }
    """
    print("\n" + "=" * 60)
    print(f"üöÄ –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
    print("=" * 60)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = config.get('batch_size', 128)
    latent_dim = config.get('latent_dim', 32)
    epochs = config.get('epochs', 30)
    lr = config.get('learning_rate', 3e-4)

    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   Latent dim: {latent_dim}")
    print(f"   Batch size: {batch_size}")
    print(f"   –≠–ø–æ—Ö–∏: {epochs}")
    print(f"   –ú–æ–¥–µ–ª–∏: {config.get('models', ['vae', 'focus_vae'])}")

    # –î–∞–Ω–Ω—ã–µ
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\nüì• –î–∞–Ω–Ω—ã–µ: {len(train_dataset)} train, {len(test_dataset)} test")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'config': config,
        'device': str(device),
        'models': {}
    }

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models_to_train = config.get('models', ['vae', 'iwae', 'vamp', 'focus_vae'])

    for model_name in models_to_train:
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ: {model_name}")
        print("-" * 40)

        if model_name == 'vae':
            model = VAE(latent_dim)
        elif model_name == 'iwae':
            model = IWAE(latent_dim)
        elif model_name == 'vamp':
            model = VampPriorVAE(latent_dim)
        elif model_name == 'focus_vae':
            model = FocusVAE(latent_dim)
        else:
            print(f"   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            continue

        # –û–±—É—á–µ–Ω–∏–µ
        losses = train_model(model, train_loader, epochs, lr, device)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_loss = evaluate_model(model, test_loader, device)

        print(f"   ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π Train Loss: {losses[-1]:.2f}")
        print(f"   ‚úÖ Test Loss: {test_loss:.2f}")

        results['models'][model_name] = {
            'train_losses': losses,
            'test_loss': test_loss,
            'final_train_loss': losses[-1]
        }

    print("\n" + "=" * 60)
    print(f"‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 60)

    return results
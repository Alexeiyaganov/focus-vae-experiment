"""
VAE Experiment - Training Module
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
import json
from pathlib import Path
import gc



def check_memory(stage=""):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        cached = torch.cuda.memory_reserved() / 1024**2
        print(f"   üìä GPU –ø–∞–º—è—Ç—å {stage}: {allocated:.1f}MB / {cached:.1f}MB")




# ========== –ú–û–î–ï–õ–ò ==========
class Encoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.mu = nn.Linear(256, latent_dim)
        self.logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.net(x)
        return self.mu(h), self.logvar(h)


class Decoder(nn.Module):
    def __init__(self, latent_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 784),
            nn.Sigmoid()
        )

    def forward(self, z):
        return self.net(z)


class VAE(nn.Module):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π VAE"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon, x, mu, logvar):
        BCE = nn.functional.binary_cross_entropy(recon, x.view(-1, 784), reduction='sum')
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return (BCE + KLD) / x.size(0)


class FocusVAE(nn.Module):
    """Focus-ELBO VAE - –ù–∞—à –º–µ—Ç–æ–¥"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=5, beta=0.001):
        mu_0, logvar_0 = self.encoder(x.view(-1, 784))
        batch_size = mu_0.size(0)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        mu = mu_0.unsqueeze(0).expand(k, -1, -1).clone()
        logvar = logvar_0.unsqueeze(0).expand(k, -1, -1)

        # –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞
        with torch.no_grad():
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            z = mu + eps * std

            recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
            x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            mse = ((recon - x_exp) ** 2).mean(dim=-1)
            weights = torch.softmax(-mse * beta, dim=0)

            # –°–¥–≤–∏–≥ —Å—Ä–µ–¥–Ω–µ–≥–æ
            delta = (weights.unsqueeze(-1) * (z - mu)).sum(dim=0)
            mu = mu + 0.1 * delta.unsqueeze(0)

        # –§–∏–Ω–∞–ª—å–Ω—ã–π IWAE loss
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        eps_stable = 1e-8

        log_p_x = -nn.functional.binary_cross_entropy(
            recon, x_exp, reduction='none'
        ).sum(dim=-1)

        log_p_z = -0.5 * (z ** 2).sum(dim=-1)
        log_q_z = -0.5 * (
                logvar + (z - mu).pow(2) / (logvar.exp() + eps_stable)
        ).sum(dim=-1)

        log_weight = log_p_x + log_p_z - log_q_z

        # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)
        weight = torch.exp(log_weight - max_log_weight)
        normalized_weight = weight / (weight.sum(dim=0, keepdim=True) + eps_stable)

        loss = -torch.sum(normalized_weight * log_weight, dim=0).mean()

        # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        min_loss = 50.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π loss –¥–ª—è MNIST
        if loss < min_loss:
            print(f"   ‚ö†Ô∏è FocusVAE loss —Å–ª–∏—à–∫–æ–º –º–∞–ª ({loss:.2f}), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è IWAE loss")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π IWAE loss –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π
            loss = -torch.log(weight.mean(dim=0) + eps_stable).mean()

        return loss


class IWAE(nn.Module):
    """Importance Weighted Autoencoder"""

    def __init__(self, latent_dim=32):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim

    def loss(self, x, k=5):
        mu, logvar = self.encoder(x.view(-1, 784))
        batch_size = mu.size(0)

        # –†–∞—Å—à–∏—Ä—è–µ–º –¥–ª—è k —Å—ç–º–ø–ª–æ–≤
        mu = mu.unsqueeze(0).expand(k, -1, -1)
        logvar = logvar.unsqueeze(0).expand(k, -1, -1)

        # –°—ç–º–ø–ª–∏—Ä—É–µ–º
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
        recon = self.decoder(z.view(-1, self.latent_dim)).view(k, batch_size, -1)
        x_exp = x.view(-1, 784).unsqueeze(0).expand(k, -1, -1)

        # log p(x|z) - –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–ª–∏
        log_p_x = -nn.functional.binary_cross_entropy(
            recon, x_exp, reduction='none'
        ).sum(dim=-1)  # [k, batch]

        # log p(z) - –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–µ–Ω –∫–æ–¥ –≤ prior
        log_p_z = -0.5 * (z ** 2).sum(dim=-1)  # [k, batch]

        # log q(z|x) - –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä–æ—è—Ç–µ–Ω –∫–æ–¥ –≤ —ç–Ω–∫–æ–¥–µ—Ä–µ
        log_q_z = -0.5 * (
                logvar + (z - mu).pow(2) / logvar.exp()
        ).sum(dim=-1)  # [k, batch]

        # –í–µ—Å–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏
        log_weight = log_p_x + log_p_z - log_q_z

        # –°—Ç–∞–±–∏–ª—å–Ω—ã–π LogSumExp
        max_log_weight, _ = torch.max(log_weight, dim=0, keepdim=True)

        # –í—ã—á–∏—Ç–∞–µ–º –º–∞–∫—Å–∏–º—É–º –¥–ª—è —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        weight = torch.exp(log_weight - max_log_weight)

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ—Å–∞
        normalized_weight = weight / (weight.sum(dim=0, keepdim=True) + 1e-8)

        # IWAE loss
        loss = -torch.sum(normalized_weight * log_weight, dim=0).mean()

        # –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ NaN
        if torch.isnan(loss):
            print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω NaN –≤ IWAE loss")
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Å—Ç–æ–π loss
            loss = torch.tensor(100.0, device=x.device, requires_grad=True)

        return loss


class VampPriorVAE(nn.Module):
    """VampPrior - Variational Mixture of Posteriors"""

    def __init__(self, latent_dim=32, num_components=20):
        super().__init__()
        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder(latent_dim)
        self.latent_dim = latent_dim
        self.num_components = num_components

        # –ü—Å–µ–≤–¥–æ-–≤—Ö–æ–¥—ã (–æ–±—É—á–∞–µ–º—ã–µ)
        self.pseudo_inputs = nn.Parameter(torch.randn(num_components, 784))
        self.pseudo_encoder = Encoder(latent_dim)

    def forward(self, x):
        mu, logvar = self.encoder(x)
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decoder(z), mu, logvar

    def loss(self, recon_x, x, mu, logvar):
        try:
            BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
            print(f"   ‚úÖ BCE –≤—ã—á–∏—Å–ª–µ–Ω: {BCE.item():.2f}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ BCE: {e}")
            raise

        try:
            # –ü–æ–ª—É—á–∞–µ–º prior –∏–∑ –ø—Å–µ–≤–¥–æ-–≤—Ö–æ–¥–æ–≤
            pseudo_mu, pseudo_logvar = self.pseudo_encoder(self.pseudo_inputs)
            print(f"   ‚úÖ pseudo_mu shape: {pseudo_mu.shape}, pseudo_logvar shape: {pseudo_logvar.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ pseudo_encoder: {e}")
            raise

        batch_size = mu.size(0)
        print(f"   üìä batch_size: {batch_size}, latent_dim: {self.latent_dim}, num_components: {self.num_components}")

        try:
            # –†–∞—Å—à–∏—Ä—è–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ broadcasting
            mu_expanded = mu.unsqueeze(1)  # [batch_size, 1, latent_dim]
            print(f"   ‚úÖ mu_expanded shape: {mu_expanded.shape}")

            pseudo_mu_expanded = pseudo_mu.unsqueeze(0)  # [1, num_components, latent_dim]
            print(f"   ‚úÖ pseudo_mu_expanded shape: {pseudo_mu_expanded.shape}")

            pseudo_logvar_expanded = pseudo_logvar.unsqueeze(0)
            print(f"   ‚úÖ pseudo_logvar_expanded shape: {pseudo_logvar_expanded.shape}")

            logvar_expanded = logvar.unsqueeze(1)  # [batch_size, 1, latent_dim]
            print(f"   ‚úÖ logvar_expanded shape: {logvar_expanded.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {e}")
            raise

        try:
            # –í—ã—á–∏—Å–ª—è–µ–º log q(z) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
            diff = (mu_expanded - pseudo_mu_expanded)
            print(f"   ‚úÖ diff shape: {diff.shape}")

            variance = pseudo_logvar_expanded.exp()
            print(f"   ‚úÖ variance shape: {variance.shape}")

            term2 = diff.pow(2) / variance
            print(f"   ‚úÖ term2 shape: {term2.shape}")

            log_q_components = -0.5 * torch.sum(
                logvar_expanded + term2 + pseudo_logvar_expanded,
                dim=2
            )
            print(f"   ‚úÖ log_q_components shape: {log_q_components.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}")
            raise

        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥ —Å–º–µ—Å–∏ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞)
            mix_weights = torch.ones(self.num_components, device=mu.device) / self.num_components
            log_q_components = log_q_components + torch.log(mix_weights)
            print(f"   ‚úÖ –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤: {log_q_components.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤: {e}")
            raise

        try:
            # –õ–æ–≥–∞—Ä–∏—Ñ–º —Å—É–º–º—ã —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è log q(z)
            log_q = torch.logsumexp(log_q_components, dim=1)
            print(f"   ‚úÖ log_q shape: {log_q.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ logsumexp: {e}")
            raise

        try:
            # –í—ã—á–∏—Å–ª—è–µ–º log p(z) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π prior
            two_pi = torch.full((1,), 2 * np.pi, device=mu.device)
            log_p = -0.5 * torch.sum(logvar + mu.pow(2) + torch.log(two_pi), dim=1)
            print(f"   ‚úÖ log_p shape: {log_p.shape}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ log_p: {e}")
            raise

        try:
            # KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
            KLD = (log_q - log_p).sum()
            print(f"   ‚úÖ KLD: {KLD.item():.2f}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ KLD: {e}")
            raise

        total_loss = (BCE + KLD) / x.size(0)
        print(f"   ‚úÖ Total loss: {total_loss.item():.2f}")

        return total_loss


# ========== –û–ë–£–ß–ï–ù–ò–ï ==========
def train_model(model, train_loader, epochs=30, lr=3e-4, device='cuda'):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    losses = []
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        for batch_idx, (data, _) in enumerate(train_loader):
            data = data.to(device)
            optimizer.zero_grad()

            # –†–∞–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)

            elif isinstance(model, IWAE):
                loss = model.loss(data, k=5)  # IWAE —Ç–æ–ª—å–∫–æ —Å k

            elif isinstance(model, VampPriorVAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)  # VampPrior –∫–∞–∫ VAE

            elif isinstance(model, FocusVAE):
                loss = model.loss(data, k=5, beta=0.001)  # FocusVAE —Å beta

            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        losses.append(avg_loss)

        if (epoch + 1) % 10 == 0:
            print(f"      –≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, Loss: {avg_loss:.2f}")

    return losses


# ========== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ==========
def evaluate_model(model, test_loader, device='cuda'):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)

            # –†–∞–∑–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            if isinstance(model, VAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)

            elif isinstance(model, IWAE):
                loss = model.loss(data, k=5)

            elif isinstance(model, VampPriorVAE):
                recon, mu, logvar = model(data.view(-1, 784))
                loss = model.loss(recon, data, mu, logvar)  # VampPrior –∫–∞–∫ VAE

            elif isinstance(model, FocusVAE):
                loss = model.loss(data, k=3, beta=0.01)  # FocusVAE —Å beta

            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")

            total_loss += loss.item()

    return total_loss / len(test_loader)


# ========== –û–°–ù–û–í–ù–û–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ ==========
def run_experiment(config):
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    config: {
        'epochs': 30,
        'batch_size': 128,
        'latent_dim': 32,
        'learning_rate': 3e-4,
        'models': ['vae', 'focus_vae']
    }
    """
    print("\n" + "=" * 60)
    print(f"üöÄ –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
    print("=" * 60)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = config.get('batch_size', 128)
    latent_dim = config.get('latent_dim', 32)
    epochs = config.get('epochs', 30)
    lr = config.get('learning_rate', 3e-4)

    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   Latent dim: {latent_dim}")
    print(f"   Batch size: {batch_size}")
    print(f"   –≠–ø–æ—Ö–∏: {epochs}")
    print(f"   –ú–æ–¥–µ–ª–∏: {config.get('models', ['vae', 'focus_vae'])}")

    # –î–∞–Ω–Ω—ã–µ
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\nüì• –î–∞–Ω–Ω—ã–µ: {len(train_dataset)} train, {len(test_dataset)} test")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'config': config,
        'device': str(device),
        'models': {}
    }

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models_to_train = config.get('models', ['vae', 'iwae', 'vamp', 'focus_vae'])

    for model_name in models_to_train:
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ: {model_name}")
        print("-" * 40)

        # ===== –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –ú–û–î–ï–õ–¨–Æ =====
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"   üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –ø–µ—Ä–µ–¥ {model_name}")
        # ==========================================

        check_memory("–¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        if model_name == 'vae':
            model = VAE(latent_dim)
        elif model_name == 'iwae':
            model = IWAE(latent_dim)
        elif model_name == 'vamp':
            model = VampPriorVAE(latent_dim)
        elif model_name == 'focus_vae':
            model = FocusVAE(latent_dim)
        else:
            print(f"   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            continue

        check_memory("–ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")

        # –û–±—É—á–µ–Ω–∏–µ
        losses = train_model(model, train_loader, epochs, lr, device)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_loss = evaluate_model(model, test_loader, device)

        print(f"   ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π Train Loss: {losses[-1]:.2f}")
        print(f"   ‚úÖ Test Loss: {test_loss:.2f}")

        results['models'][model_name] = {
            'train_losses': losses,
            'test_loss': test_loss,
            'final_train_loss': losses[-1]
        }

        check_memory("–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")

        # ===== –£–î–ê–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï =====
        del model  # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å
        gc.collect()  # –°–æ–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä Python
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
            print(f"   üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ {model_name}")
        # =================================================

        check_memory("–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è")

    print("\n" + "=" * 60)
    print(f"‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 60)

    return results